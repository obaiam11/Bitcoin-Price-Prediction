{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CS 230 Final Project\n",
    "# Scott Keene, Dante Zakhidov, Abdulmalik Obaid\n",
    "# Bitcoin Price Prediction\n",
    "\n",
    "# Type the directory that the data files are located\n",
    "directory = \"/Users/scottkeene/Documents/Python/CS 230 Project/input\"\n",
    "# Type the filename of the bitcoin price data file (starting with /)\n",
    "bitcoin_data_filename = \"/coinbaseUSD_1-min_data_2014-12-01_to_2018-01-08 2.csv\"\n",
    "# Type the filename of the Google Trend data file (starting with /)\n",
    "google_data_filename = \"/Google Search Frequency.csv\"\n",
    "\n",
    "# Import these at the beginning of every session!\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn import preprocessing, cross_validation, svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", directory]).decode(\"utf8\"))\n",
    "\n",
    "# Import packages for keras\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "from keras.initializers import glorot_uniform\n",
    "import scipy.misc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in bitcoin pricing data from file\n",
    "def import_data_file(directory, bitcoin_data_filename):\n",
    "    # Get raw data\n",
    "    BTC_Price = pd.read_csv(directory + bitcoin_data_filename)\n",
    "    \n",
    "    # Identify the Price variance for the each days\n",
    "    # Variance = Close price minus the Open price \n",
    "    # Negative value indicate price has declined for that day and Positive value represent increase in price\n",
    "    BTC_Price['Variance'] = ((BTC_Price[\"Close\"] - BTC_Price[\"Open\"])/BTC_Price[\"Close\"])*100\n",
    "\n",
    "    # Frequeny of change for a given day (High - Low)\n",
    "    BTC_Price['Freq'] = ((BTC_Price[\"High\"] - BTC_Price[\"Low\"])/BTC_Price[\"High\"])*100\n",
    "    \n",
    "    # Create binary classification\n",
    "    BTC_Price['Up_Label'] = (BTC_Price[\"Close\"] > BTC_Price[\"Open\"])\n",
    "    \n",
    "    return BTC_Price\n",
    "\n",
    "BTC_Price.head() # verify data looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the minute data to hour by hour data\n",
    "def generate_hourly_dataset(dataset, num_hours):\n",
    "    \n",
    "    data = dataset.as_matrix()\n",
    "    min_to_hour = 60\n",
    "    X = np.zeros((num_hours, 5))\n",
    "    initial_timestamp = data[0,0]\n",
    "    for i in range(0, num_hours):\n",
    "        X[i,0] = initial_timestamp + min_to_hour*60*i\n",
    "        X[i,1] = data[i*min_to_hour, 1] #Open\n",
    "        X[i,2] = np.max(data[i*min_to_hour:(i + 1)*min_to_hour, 2]) #High\n",
    "        X[i,3] = np.min(data[i*min_to_hour:(i + 1)*min_to_hour, 3]) #Low\n",
    "        X[i,4] = data[(i+1)*min_to_hour, 1] #Close\n",
    "    X_data = pd.DataFrame(X, columns = [\"Timestamp\", \"Open\", \"High\", \"Low\", \"Close\"])\n",
    "    \n",
    "    X_data['Variance'] = ((X_data[\"Close\"] - X_data[\"Open\"])/X_data[\"Close\"])*100\n",
    "\n",
    "    # Frequeny of change for a given day (High - Low)\n",
    "    X_data['Freq'] = ((X_data[\"High\"] - X_data[\"Low\"])/X_data[\"High\"])*100\n",
    "    \n",
    "    # Create binary classification\n",
    "    X_data['Up_Label'] = (X_data[\"Close\"] > X_data[\"Open\"])\n",
    "    \n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the minute data to day by day data\n",
    "def generate_daily_dataset(dataset, num_days):\n",
    "    \n",
    "    data = dataset.as_matrix()\n",
    "    \n",
    "    min_to_day = 1440\n",
    "    X = np.zeros((num_days, 5))\n",
    "    initial_timestamp = data[0,0]\n",
    "    for i in range(0, num_days):\n",
    "        X[i,0] = data[i*min_to_day, 0]\n",
    "        X[i,1] = data[i*min_to_day, 1] #Open\n",
    "        X[i,2] = np.max(data[i*min_to_day:(i + 1)*min_to_day, 2]) #High\n",
    "        X[i,3] = np.min(data[i*min_to_day:(i + 1)*min_to_day, 3]) #Low\n",
    "        X[i,4] = data[(i+1)*min_to_day, 1] #Close\n",
    "    X_data = pd.DataFrame(X, columns = [\"Timestamp\", \"Open\", \"High\", \"Low\", \"Close\"])\n",
    "    \n",
    "    X_data['Variance'] = ((X_data[\"Close\"] - X_data[\"Open\"])/X_data[\"Close\"])*100\n",
    "\n",
    "    # Frequeny of change for a given day (High - Low)\n",
    "    X_data['Freq'] = ((X_data[\"High\"] - X_data[\"Low\"])/X_data[\"High\"])*100\n",
    "    \n",
    "    search_history = pd.read_csv(directory + google_data_filename)\n",
    "    \n",
    "    return X_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# no longer used, but you can use it to normalize the data. Used for simple NN\n",
    "def normalize(data):\n",
    "    \n",
    "    column_sum = data.sum(axis = 0)\n",
    "    data = data/column_sum\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Baseline simple neural network ## 2 layers, for binary classification of if price will go up or down\n",
    "# Import required functions from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization, LSTM\n",
    "\n",
    "# Get data file\n",
    "BTC_Price = import_data_file(directory, bitcoin_data_filename)\n",
    "timesteps = 10\n",
    "\n",
    "X, Y, X_dev, Y_dev = generate_min_dataset(BTC_Price, 100000, 1000, start_index = 40000) # change generate_min_dataset to generate_day_dataset to compare different sampling frequencies\n",
    "X = normalize(X)\n",
    "X_dev = normalize(X_dev)\n",
    "batch_size = 32\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(8, output_dim=256))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model with batch sizes of 32 samples\n",
    "model.fit(X, Y, epochs = 2, batch_size = 100)\n",
    "score = model.evaluate(X_dev, Y_dev, batch_size=100)\n",
    "print(model.metrics_names + score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now, we want to test the performance of this vs. a simple lag model (no NN)\n",
    "#Creating a new Panda Database with just timestamp and up_label \n",
    "Baseline_Database_min = BTC_Price[['Timestamp', 'Open', 'Close','Up_Label']].copy()\n",
    "Baseline_Database_day = X_day[['Timestamp','Open','Close','Up_Label']].copy()\n",
    "Baseline_Database_min.head()\n",
    "\n",
    "# plotting bitcoin price vs day\n",
    "plt.plot(BTC_Price['Timestamp'],BTC_Price['Close'],linewidth=1.0)\n",
    "plt.axis([1509494400, 1511913599,5000,11000])\n",
    "plt.ylabel('Closing Price($)',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 Minute LAG Model\n",
    "#Calculating Accuracy of Lag Model to Test Polarity Baseline of Minute Database\n",
    "#Calculating 1 day lag shift\n",
    "Baseline_Database_min['shift'] = Baseline_Database_min['Up_Label'].shift(1) #new column with values of up_label shifted by 1 \n",
    "Baseline_Database_min['lag_correct'] = np.where(Baseline_Database_min['Up_Label'] == Baseline_Database_min['shift'],1,0)\n",
    "acc_min = Baseline_Database_min['lag_correct'].sum()/Baseline_Database_min['lag_correct'].size*100  #outputs accuracy\n",
    "acc_lag_min = np.repeat(acc,15)\n",
    "\n",
    "# Plotting Lag Model over 25 minutes to depict Lag Model\n",
    "plt.plot(Baseline_Database_min['Timestamp'],Baseline_Database_min['Up_Label'],color = 'b', linewidth=1.0)\n",
    "plt.plot(Baseline_Database_min['Timestamp'],Baseline_Database_min['shift'],color = 'g', linewidth=1.0)\n",
    "plt.axis([1487512980, 1487513980,-0.1,1.1]) #Day\n",
    "plt.axis()\n",
    "plt.ylabel('Polarity Index',fontsize=12)\n",
    "plt.title('Lag Model',fontsize=16)\n",
    "plt.show()\n",
    "print(acc_min)\n",
    "\n",
    "Baseline_Database_min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 DAY Lag Model\n",
    "# Calculating Accuracy of Lag Model to Test Polarity Baseline of Day Database\n",
    "# Will compare to NN model and exponentially weighted moving average\n",
    "#Calculating 1 day lag shift (NO BINNING)\n",
    "Baseline_Database_day['shift'] = Baseline_Database_day['Up_Label'].shift(1) #new column with values of up_label shifted by 1 \n",
    "Baseline_Database_day['lag_correct'] = np.where(Baseline_Database_day['Up_Label'] == Baseline_Database_day['shift'],1,0)\n",
    "acc_day = Baseline_Database_day['lag_correct'].sum()/Baseline_Database_day['lag_correct'].size*100  #outputs accuracy\n",
    "#acc_lag_day = np.repeat(acc,15)\n",
    "\n",
    "# Plotting Lag Model over 25 minutes to depict Lag Model\n",
    "plt.plot(Baseline_Database_day['Timestamp'],Baseline_Database_day['Up_Label'],color = 'b', linewidth=1.0)\n",
    "plt.plot(Baseline_Database_day['Timestamp'],Baseline_Database_day['shift'],color = 'g', linewidth=1.0)\n",
    "plt.axis([1487512980, 1488512980,-0.1,1.1]) #Day\n",
    "plt.axis()\n",
    "plt.ylabel('Polarity Index',fontsize=12)\n",
    "plt.title('One Day Lag Model',fontsize=16)\n",
    "plt.show()\n",
    "print(acc_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating the Exponentially Weighted Moving Average. \n",
    "# For Min dataset, binary classification of if price went up/down\n",
    "acc_ewma_min = []\n",
    "for t in range(1,16):\n",
    "    Baseline_Database_min['EWMA'] = pd.ewma(Baseline_Database_min['Up_Label'], span=t)\n",
    "    Baseline_Database_min['EWMA'] = np.where(Baseline_Database_min['EWMA'] >= 0.5,1,0)\n",
    "    Baseline_Database_min['EWMA'] = Baseline_Database_min['EWMA'].shift(1)\n",
    "    Baseline_Database_min['EWMA_correct'] = np.where(Baseline_Database_min['Up_Label'] == Baseline_Database_min['EWMA'],1,0)\n",
    "    acc_t = Baseline_Database_min['EWMA_correct'].sum()/Baseline_Database_min['EWMA_correct'].size*100\n",
    "    acc_ewma_min.append(acc_t)\n",
    "\n",
    "# plot performance of exponentially weighted moving average for min dataset\n",
    "print(acc_ewma_min)\n",
    "plt.plot(range(1,16),acc_ewma_min, color = 'b',linewidth = 1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculating the Exponentially Weighted Moving Average\n",
    "# For Day dataset, binary classification of if price went up/down\n",
    "acc_ewma_day_nobin = []\n",
    "for t in range(1,150):\n",
    "    Baseline_Database_day['EWMA'] = pd.ewma(Baseline_Database_day['Up_Label'], span=t)\n",
    "    Baseline_Database_day['EWMA'] = np.where(Baseline_Database_day['EWMA'] >= 0.5,1,0)\n",
    "    Baseline_Database_day['EWMA'] = Baseline_Database_day['EWMA'].shift(1)\n",
    "    Baseline_Database_day['EWMA_correct'] = np.where(Baseline_Database_day['Up_Label'] == Baseline_Database_day['EWMA'],1,0)\n",
    "    acc_t = Baseline_Database_day['EWMA_correct'].sum()/Baseline_Database_day['EWMA_correct'].size*100\n",
    "    acc_ewma_day_nobin.append(acc_t)\n",
    "\n",
    "# plot performance of exponentially weighted moving average for daily dataset\n",
    "\n",
    "print(acc_ewma_day_nobin)\n",
    "plt.plot(range(1,150),acc_ewma_day_nobin, color = 'b',linewidth = 1.0)\n",
    "plt.xlabel('Number of Days Considered',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For Min dataset, binnined labelling of the data to see how much the price fluctuated\n",
    "# Calculating the Exponentially Weighted Moving Average\n",
    "acc_ewma_min = []\n",
    "delta = Baseline_Database_min[\"Close\"] - Baseline_Database_min[\"Open\"]\n",
    "Baseline_Database_min['percent_delta'] = np.divide(delta, Baseline_Database_min[\"Open\"])\n",
    "Y_data_min = generate_binned_labels_custom(Baseline_Database_min['percent_delta'], binner_settings = [-0.025, -0.01, -0.003, 0.003, 0.01, 0.025])\n",
    "for t in range(1,2):\n",
    "    percent_delta = pd.ewma(Baseline_Database_min['percent_delta'], span=t).shift(1)\n",
    "    Y_data_t = generate_binned_labels_custom(percent_delta, binner_settings = [-0.025, -0.01, -0.003, 0.003, 0.01, 0.025])\n",
    "    Baseline_Database_min['EWMA_correct'] = np.where(np.sum(np.abs(np.subtract(Y_data_min,Y_data_t)),axis=1)==0,1,0)\n",
    "    acc_t = Baseline_Database_day['EWMA_correct'].sum()/Baseline_Database_day['EWMA_correct'].size*100\n",
    "    acc_ewma_min.append(acc_t)\n",
    "\n",
    "print(acc_ewma_min)\n",
    "plt.plot(range(1,2),acc_ewma_min, color = 'b',linewidth = 1.0)\n",
    "plt.show()\n",
    "\n",
    "Y_data_min.to_csv(\"C:\\\\Users\\\\Dante\\\\Documents\\\\Stanford\\\\Coursework\\\\Winter 2018\\\\CS230\\\\Project\\\\Y_data.csv\")\n",
    "Y_data_min = pd.read_csv(\"C:\\\\Users\\\\Dante\\\\Documents\\\\Stanford\\\\Coursework\\\\Winter 2018\\\\CS230\\\\Project\\\\Y_data.csv\")\n",
    "names = Y_data_min.columns.values\n",
    "Y_data_min = Y_data_min.drop(['Unnamed: 0'], axis =1)\n",
    "binner_settings = [-0.025, -0.01, -0.003, 0.003, 0.01, 0.025]\n",
    "plt.bar(linspace(0, len(binner_settings), len(binner_settings)+1), np.sum(Y_data_min, axis = 0)/np.sum(np.sum(Y_data_min, axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For Day dataset, binnined labelling of the data to see how much the price fluctuated\n",
    "#Calculating the Exponentially Weighted Moving Average\n",
    "acc_ewma_day = []\n",
    "delta = Baseline_Database_day[\"Close\"] - Baseline_Database_day[\"Open\"]\n",
    "Baseline_Database_day['percent_delta'] = np.divide(delta, Baseline_Database_day[\"Open\"])\n",
    "Y_data = generate_binned_labels_custom(Baseline_Database_day['percent_delta'], binner_settings = [-0.025, -0.01, -0.003, 0.003, 0.01, 0.025])\n",
    "for t in range(1,16):\n",
    "    percent_delta = pd.ewma(Baseline_Database_day['percent_delta'], span=t).shift(1)\n",
    "    Y_data_t = generate_binned_labels_custom(percent_delta, binner_settings = [-0.025, -0.01, -0.003, 0.003, 0.01, 0.025])\n",
    "    Baseline_Database_day['EWMA_correct'] = np.where(np.sum(np.abs(np.subtract(Y_data,Y_data_t)),axis=1)==0,1,0)\n",
    "    acc_t = Baseline_Database_day['EWMA_correct'].sum()/Baseline_Database_day['EWMA_correct'].size*100\n",
    "    acc_ewma_day.append(acc_t)\n",
    "\n",
    "acc_lag_day = np.repeat(acc_ewma_day[0],15)    \n",
    "print(acc_ewma_day, acc_lag_day)\n",
    "plt.plot(range(1,16),acc_ewma_day, color = 'b',linewidth = 2.0)\n",
    "plt.xlabel('Number of Days Considered',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "#plt.title('Baseline: Exponentially Weighted Moving Average',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plotting accuracy vs. length of weighted average\n",
    "t = np.arange(1,16)\n",
    "plt.plot(t,acc_ewma_min, color = 'b',linewidth = 2.0)\n",
    "plt.plot(t,acc_lag_min, color = 'g', linewidth = 2.0)\n",
    "plt.plot\n",
    "plt.xlabel('Length of Span',fontsize=12)\n",
    "plt.ylabel('Accuracy',fontsize=12)\n",
    "plt.title('Exponentially Weighted Moving Average',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# With a good understanding of the baseline performance, back to NN. \n",
    "# Generating binned data, to predict the amount price fluctuations will be\n",
    "def generate_binned_labels_V2(X, number_of_bins, max_bin):\n",
    "    \n",
    "    delta = X[\"Close\"] - X[\"Open\"]\n",
    "    percent_delta = np.divide(delta, X[\"Open\"])\n",
    "    Y = np.zeros((len(X[\"Open\"]), number_of_bins))\n",
    "    bin_range = 2*max_bin/(number_of_bins-2)\n",
    "    columns = []\n",
    "    ## Generate Y as a one hot vector\n",
    "    for i in range(0, len(X[\"Open\"])):\n",
    "        if percent_delta[i] < -max_bin:\n",
    "            Y[i, 0] = 1\n",
    "        elif percent_delta[i] >= max_bin:\n",
    "            Y[i, number_of_bins-1] = 1\n",
    "        else:    \n",
    "            for j in range(1, number_of_bins-1):\n",
    "                if percent_delta[i] >= -max_bin + bin_range*(j-1) and percent_delta[i] < -max_bin + bin_range*j:\n",
    "                    Y[i, j] = 1\n",
    "    columns.append(\"<\" + str(-max_bin) + \"%\")            \n",
    "    for k in range(1, number_of_bins-1):\n",
    "        columns.append(str(-max_bin + bin_range*(k-1)) + \"% to \" + str(-max_bin + bin_range*k) + \"%\")\n",
    "    columns.append(\">\" + str(max_bin) + \"%\")\n",
    "    Y_data = pd.DataFrame(Y, columns = columns)    \n",
    "    return Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 4 layer NN with softmax activation, used for 'binned' data to predict how much the price will fluctuate\n",
    "def price_data_simple_model(timesteps, input_dim, n_a, number_of_bins):\n",
    "    # Import required functions from Keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, BatchNormalization, LSTM\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_a, return_sequences = True, input_shape = (timesteps, input_dim)))\n",
    "    model.add(LSTM(n_a, return_sequences = True))\n",
    "    model.add(LSTM(n_a))\n",
    "    model.add(Dense(number_of_bins, activation = 'softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get data file\n",
    "BTC_Price = import_data_file(directory, bitcoin_data_filename)\n",
    "\n",
    "X_daily = generate_daily_dataset(BTC_Price, 1093)\n",
    "Y_daily = generate_binned_labels_custom(X_daily)\n",
    "\n",
    "X = X_daily.as_matrix()\n",
    "Y = Y_daily.as_matrix()\n",
    "\n",
    "Y_day.head() # verify data looks ok\n",
    "\n",
    "# Run model\n",
    "timesteps = 20\n",
    "X = rearrange_training_data(X[:,1:6], timesteps)\n",
    "X_train = X[0:999,:,:]\n",
    "X_dev = X[1000:1049,:,:]\n",
    "\n",
    "Y_train = Y[1+timesteps:1000+timesteps,:]\n",
    "Y_dev = Y[1001+timesteps:1050+timesteps,:]\n",
    "model = price_data_model(timesteps, 6, 32, 6)\n",
    "model.summary()\n",
    "model.fit(X_train, Y_train, epochs = 10, validation_data=(X_dev, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot distrubution of price fluctuations\n",
    "plt.bar([1, 2, 3, -1, -2, -3], Y.sum(axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Inputs ## Parameters to tune. These work reasonably well\n",
    "timesteps = 8\n",
    "bins = 5\n",
    "max_bin = 0.1\n",
    "n_a = 64\n",
    "X_width = 6\n",
    "batch_size = 32\n",
    "\n",
    "X_daily = generate_daily_dataset(BTC_Price, 1093)\n",
    "Y_daily = generate_binned_labels_V2(X_daily, bins, max_bin)\n",
    "\n",
    "X = X_daily.as_matrix()\n",
    "Y = Y_daily.as_matrix()\n",
    "\n",
    "m = int((floor(1000/batch_size))*batch_size)\n",
    "n = int((floor(93/batch_size))*batch_size)\n",
    "\n",
    "X = rearrange_training_data(X[:,1:X_width], timesteps)\n",
    "X_train = X[0:m,:,:]\n",
    "X_dev = X[m+1:m+n+1,:,:]\n",
    "\n",
    "Y_train = Y[1+timesteps:1+m+timesteps,:]\n",
    "Y_dev = Y[m+2+timesteps:m+2+n+timesteps,:]\n",
    "\n",
    "model = price_data_binned_model(timesteps, X_width, n_a, bins, batch_size)\n",
    "model.summary()\n",
    "model.fit(X_train, Y_train, epochs = 100, batch_size = batch_size, validation_data=(X_dev, Y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Model\n",
    "#  Add more layers to the model\n",
    "def price_data_binned_model(timesteps, input_dim, n_a, number_of_bins, batch_size, loss):\n",
    "    # Import required functions from Keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, BatchNormalization, LSTM\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(batch_input_shape = (batch_size, timesteps, input_dim)))\n",
    "    model.add(LSTM(n_a, return_sequences = True, stateful = True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(n_a, return_sequences = True))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(n_a))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(number_of_bins, activation = 'softmax'))\n",
    "    model.compile(loss = loss, optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate binned data to predict amount of price fluctuations. This was to test the ability to predict large fluctuations\n",
    "def generate_binned_labels_log_spacing(X, number_of_bins, max_bin, bin_range):\n",
    "    \n",
    "    delta = X[\"Close\"] - X[\"Open\"]\n",
    "    percent_delta = np.divide(delta, X[\"Open\"])\n",
    "    Y = np.zeros((len(X[\"Open\"]), number_of_bins))\n",
    "    bin_range = 2*max_bin/(number_of_bins-2)\n",
    "    columns = []\n",
    "    spacing = np.logspace(log10(max_bin)-bin_range, log10(max_bin), number_of_bins/2-1)\n",
    "    spacing = (np.append(np.append(spacing[::-1], 0),(-1*spacing)))[::-1]\n",
    "    \n",
    "    ## Generate Y as a one hot vector\n",
    "    for i in range(0, len(X[\"Open\"])):\n",
    "        if percent_delta[i] < -max_bin:\n",
    "            Y[i, 0] = 1\n",
    "        elif percent_delta[i] >= max_bin:\n",
    "            Y[i, number_of_bins-1] = 1\n",
    "        else:    \n",
    "            for j in range(1, number_of_bins-1):\n",
    "                if percent_delta[i] >= spacing[j-1] and percent_delta[i] < spacing[j]:\n",
    "                    Y[i, j] = 1\n",
    "    columns.append(\"<\" + str(-max_bin) + \"%\")            \n",
    "    for k in range(1, number_of_bins-1):\n",
    "        columns.append(str(spacing[k-1]) + \"% to \" + str(spacing[k]) + \"%\")\n",
    "    columns.append(\">\" + str(max_bin) + \"%\")\n",
    "    Y_data = pd.DataFrame(Y, columns = columns)    \n",
    "    return Y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate binned data. This is the one we stuck with that works best. This distrubution is even across the last 3 years of bitcon pricing data\n",
    "def generate_binned_labels_custom(percent_delta, binner_settings = [-0.025, -0.01, -0.003, 0.003, 0.01, 0.025]):\n",
    "    \n",
    "    number_of_bins = len(binner_settings)+1\n",
    "    Y = np.zeros((len(percent_delta), number_of_bins))\n",
    "    columns = []\n",
    "    \n",
    "    ## Generate Y as a one hot vector\n",
    "    for i in range(0, len(percent_delta)):\n",
    "        if percent_delta[i] < binner_settings[0]:\n",
    "            Y[i, 0] = 1\n",
    "        elif percent_delta[i] >= binner_settings[number_of_bins-2]:\n",
    "            Y[i, number_of_bins-1] = 1\n",
    "        else:    \n",
    "            for j in range(1, number_of_bins-1):\n",
    "                if percent_delta[i] >= binner_settings[j-1] and percent_delta[i] < binner_settings[j]:\n",
    "                    Y[i, j] = 1\n",
    "    columns.append(\"<\" + str(binner_settings[0]) + \"%\")            \n",
    "    for k in range(1, number_of_bins-1):\n",
    "        columns.append(str(binner_settings[k-1]) + \"% to \" + str(binner_settings[k]) + \"%\")\n",
    "    columns.append(\">\" + str(binner_settings[number_of_bins-2]) + \"%\")\n",
    "    Y_data = pd.DataFrame(Y, columns = columns)    \n",
    "    return Y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell runs the extended model with the daily dataset, where fluctuations are binned based on amount\n",
    "# Overfits the data! Doesn't generalize to dev set\n",
    "## Inputs ##\n",
    "timesteps = 4\n",
    "n_a = 64\n",
    "X_width = 6\n",
    "batch_size = 32\n",
    "#bin settings\n",
    "binner_settings = [-0.025, -0.01, -0.003, 0.003, 0.01, 0.025]\n",
    "\n",
    "BTC_Price = import_data_file(bitcoin_data_filename)\n",
    "\n",
    "X_daily = generate_daily_dataset(BTC_Price, 1093)\n",
    "delta = X_daily[\"Close\"] - X_daily[\"Open\"]\n",
    "percent_delta = np.divide(delta, X_daily[\"Open\"])\n",
    "Y_daily = generate_binned_labels_custom(percent_delta, binner_settings)\n",
    "\n",
    "X = X_daily.as_matrix()\n",
    "Y = Y_daily.as_matrix()\n",
    "\n",
    "m = int((floor(800/batch_size))*batch_size)\n",
    "n = int((floor(200/batch_size))*batch_size)\n",
    "\n",
    "X = rearrange_training_data(X[:,1:X_width], timesteps)\n",
    "X_train = X[0:m,:,:]\n",
    "X_dev = X[m+1:m+n+1,:,:]\n",
    "\n",
    "Y_train = Y[1+timesteps:1+m+timesteps,:]\n",
    "Y_dev = Y[m+2+timesteps:m+2+n+timesteps,:]\n",
    "print(Y_train)\n",
    "plt.bar(linspace(0, len(binner_settings), len(binner_settings)+1), np.sum(Y, axis = 0)/np.sum(np.sum(Y, axis = 0)))\n",
    "\n",
    "model = price_data_binned_model(timesteps, X_width, n_a, len(binner_settings)+1, batch_size)\n",
    "model.summary()\n",
    "#model.fit(X_train, Y_train, epochs = 500, batch_size = batch_size, validation_data=(X_dev, Y_dev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell runs the extended model with the daily dataset with up/down labels for price fluctuation (binary)\n",
    "## Inputs ##\n",
    "timesteps = 4\n",
    "n_a = 64\n",
    "X_width = 6\n",
    "batch_size = 32\n",
    "loss = \"binary_crossentropy\"\n",
    "\n",
    "BTC_Price = import_data_file(directory, bitcoin_data_filename)\n",
    "\n",
    "X_daily = generate_daily_dataset(BTC_Price, 1093)\n",
    "Y_daily = (X_daily[\"Close\"] > X_daily[\"Open\"])\n",
    "\n",
    "X = X_daily.as_matrix()\n",
    "Y = Y_daily.as_matrix()\n",
    "\n",
    "m = int((floor(800/batch_size))*batch_size)\n",
    "n = int((floor(200/batch_size))*batch_size)\n",
    "\n",
    "X = rearrange_training_data(X[:,1:X_width], timesteps)\n",
    "X_train = X[0:m,:,:]\n",
    "X_dev = X[m+1:m+n+1,:,:]\n",
    "\n",
    "Y_train = Y[1+timesteps:1+m+timesteps]\n",
    "Y_dev = Y[m+2+timesteps:m+2+n+timesteps]\n",
    "print(Y_train)\n",
    "\n",
    "\n",
    "model = price_data_binned_model(timesteps, X_width, n_a, 1, batch_size, loss)\n",
    "model.summary()\n",
    "training = model.fit(X_train, Y_train, epochs = 500, batch_size = batch_size, validation_data=(X_dev, Y_dev))\n",
    "\n",
    "generate_plots(training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell runs the extended model with the hourly dataset with binned labels\n",
    "## Inputs ##\n",
    "timesteps = 4\n",
    "n_a = 64\n",
    "X_width = 6\n",
    "batch_size = 32\n",
    "binner_settings = [-0.025, -0.01, -0.003, 0.003, 0.01, 0.025]\n",
    "loss = \"categorical_crossentropy\"\n",
    "\n",
    "BTC_Price = import_data_file(directory, bitcoin_data_filename)\n",
    "\n",
    "X_hourly = generate_hourly_dataset(BTC_Price, 26232)\n",
    "delta = X_hourly[\"Close\"] - X_hourly[\"Open\"]\n",
    "percent_delta = np.divide(delta, X_hourly[\"Open\"])\n",
    "Y_hourly = generate_binned_labels_custom(percent_delta, binner_settings)\n",
    "\n",
    "X = X_hourly.as_matrix()\n",
    "Y = Y_hourly.as_matrix()\n",
    "\n",
    "m = int((floor(20000/batch_size))*batch_size)\n",
    "n = int((floor(6000/batch_size))*batch_size)\n",
    "\n",
    "X = rearrange_training_data(X[:,1:X_width], timesteps)\n",
    "X_train = X[0:m,:,:]\n",
    "X_dev = X[m+1:m+n+1,:,:]\n",
    "\n",
    "Y_train = Y[1+timesteps:1+m+timesteps,:]\n",
    "Y_dev = Y[m+2+timesteps:m+2+n+timesteps,:]\n",
    "\n",
    "plt.bar(linspace(0, len(binner_settings), len(binner_settings)+1), np.sum(Y, axis = 0)/np.sum(np.sum(Y, axis = 0)))\n",
    "plt.ylabel('Frequency (%)')\n",
    "plt.xlabel('Bin Number')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "model = price_data_binned_model_w_dropout(timesteps, X_width, n_a, len(binner_settings)+1, batch_size, loss, 0.5)\n",
    "model.summary()\n",
    "training = model.fit(X_train, Y_train, epochs = 500, batch_size = batch_size, validation_data=(X_dev, Y_dev))\n",
    "generate_plots(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plots to visualize model performance\n",
    "def generate_plots(training):\n",
    "    plt.plot(training.history['loss'])\n",
    "    plt.plot(training.history['val_loss'])\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Dev'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.plot(training.history['acc'])\n",
    "    plt.plot(training.history['val_acc'])\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Dev'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_plots(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Model, wanted to test deeper network\n",
    "# Add more layers to the model\n",
    "def price_data_binned_model_w_dropout(timesteps, input_dim, n_a, number_of_bins, batch_size, loss, drop_rate):\n",
    "    # Import required functions from Keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, BatchNormalization, LSTM, Dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(batch_input_shape = (batch_size, timesteps, input_dim)))\n",
    "    model.add(LSTM(n_a, return_sequences = True, stateful = True, recurrent_dropout = drop_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(n_a, return_sequences = True, recurrent_dropout = drop_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(n_a, recurrent_dropout = drop_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(number_of_bins, activation = 'softmax'))\n",
    "    model.compile(loss = loss, optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Model\n",
    "#  Add more layers to the model\n",
    "def price_data_binned_model_w_dropout_simple(timesteps, input_dim, n_a, number_of_bins, batch_size, loss, drop_rate):\n",
    "    # Import required functions from Keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, BatchNormalization, LSTM, Dropout\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(batch_input_shape = (batch_size, timesteps, input_dim)))\n",
    "    model.add(LSTM(n_a, return_sequences = False, stateful = True))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(n_a*2, activation = 'relu'))\n",
    "    model.add(Dense(n_a, activation = 'relu'))\n",
    "    model.add(Dense(number_of_bins, activation = 'softmax'))\n",
    "    model.compile(loss = loss, optimizer = 'Adam', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load in Google Trend data\n",
    "search_history = pd.read_csv(directory + google_data_filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rearrange_training_data(X, timesteps, X_width):\n",
    "    \n",
    "    days = len(X)\n",
    "    X_rearranged = zeros((days-timesteps, timesteps, X_width))\n",
    "    for d in range(0, days-timesteps):\n",
    "        X_rearranged[d, 0:timesteps, 0:X_width] = X[d:d+timesteps, 1:X_width+1]\n",
    "\n",
    "    return X_rearranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell runs the extended model with the daily dataset with binned labels and google search data\n",
    "# To test if feeding the network google search data would improve dev performance\n",
    "## Inputs ##\n",
    "timesteps = 4\n",
    "n_a = 64\n",
    "X_width = 7\n",
    "batch_size = 32\n",
    "binner_settings = [-0.025, -0.01, -0.003, 0.003, 0.01, 0.025]\n",
    "loss = \"categorical_crossentropy\"\n",
    "\n",
    "BTC_Price = import_data_file(directory, bitcoin_data_filename)\n",
    "\n",
    "X_daily = generate_daily_dataset(BTC_Price, 1093)\n",
    "delta = X_daily[\"Close\"] - X_daily[\"Open\"]\n",
    "percent_delta = np.divide(delta, X_daily[\"Open\"])\n",
    "Y_daily = generate_binned_labels_custom(percent_delta, binner_settings)\n",
    "\n",
    "search_history = pd.read_csv(directory + google_data_filename)\n",
    "google = search_history.as_matrix()\n",
    "#X_data['Up_Label'] = (X_data[\"Close\"] > X_data[\"Open\"]) # no longer used, binary classification doesn't work well\n",
    "X_daily['Google'] = google\n",
    "\n",
    "X = X_daily.as_matrix()\n",
    "Y = Y_daily.as_matrix()\n",
    "\n",
    "m = int((floor(1000/batch_size))*batch_size)\n",
    "n = int((floor(70/batch_size))*batch_size)\n",
    "\n",
    "# Add Google daily search history as an input\n",
    "search_history = pd.read_csv(directory + google_data_filename)\n",
    "X2 = rearrange_training_data(X, timesteps, X_width)\n",
    "X_train = X2[133-1-timesteps:1093-1-timesteps,:,:]\n",
    "X_dev = X2[10-1-timesteps:106-1-timesteps,:,:]\n",
    "print(len(X_dev))\n",
    "\n",
    "Y_train = Y[133:1093,:]\n",
    "Y_dev = Y[10:106,:]\n",
    "print(len(Y_dev))\n",
    "plt.bar(linspace(0, len(binner_settings), len(binner_settings)+1), np.sum(Y, axis = 0)/np.sum(np.sum(Y, axis = 0)))\n",
    "plt.ylabel('Frequency (%)')\n",
    "plt.xlabel('Bin Number')\n",
    "plt.show()\n",
    "model = price_data_binned_model_w_dropout(timesteps, X_width, n_a, len(binner_settings)+1, batch_size, loss, 0.5)\n",
    "model.summary()\n",
    "training = model.fit(X_train, Y_train, epochs = 1000, batch_size = batch_size, validation_data=(X_dev, Y_dev))\n",
    "generate_plots(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(normalize(search_history['Search History'].as_matrix()))\n",
    "plt.plot(normalize(X_daily['Open'].as_matrix()))\n",
    "#plt.ylabel('Loss')\n",
    "plt.xlabel('Time')\n",
    "plt.legend(['Google Searches', 'BTC Price'], loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "## We find that BTC price and google searches follow each other very closely. Not great as a predictive input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Model\n",
    "#  Add more layers to the model. Add regularization in an attempt to reduce variance\n",
    "def price_data_binned_model_w_dropout_and_regularization(timesteps, input_dim, n_a, number_of_bins, batch_size, loss, drop_rate):\n",
    "    # Import required functions from Keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, BatchNormalization, LSTM, Dropout\n",
    "    from keras import regularizers\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(batch_input_shape = (batch_size, timesteps, input_dim)))\n",
    "    model.add(LSTM(n_a, return_sequences = True, stateful = True, recurrent_dropout = drop_rate, activity_regularizer = regularizers.l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(n_a, return_sequences = True, recurrent_dropout = drop_rate, activity_regularizer = regularizers.l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(n_a, recurrent_dropout = drop_rate, activity_regularizer = regularizers.l2(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(number_of_bins, activation = 'softmax', activity_regularizer = regularizers.l2(0.01)))\n",
    "    model.compile(loss = loss, optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define Model\n",
    "#  Add more layers to the model. Regularizatio without dropout. Similar performance. Still big gap between training and dev performance\n",
    "def price_data_binned_model_w_regularization(timesteps, input_dim, n_a, number_of_bins, batch_size, loss):\n",
    "    # Import required functions from Keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, BatchNormalization, LSTM, Dropout\n",
    "    from keras import regularizers\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(batch_input_shape = (batch_size, timesteps, input_dim)))\n",
    "    model.add(LSTM(n_a, return_sequences = True, stateful = True, activity_regularizer = regularizers.l1(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(n_a, return_sequences = True, activity_regularizer = regularizers.l1(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(n_a, activity_regularizer = regularizers.l1(0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(number_of_bins, activation = 'softmax', activity_regularizer = regularizers.l1(0.01)))\n",
    "    model.compile(loss = loss, optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To test effect of feeding our NN with sentiment analysis of twitter data\n",
    "# Will use vaderSentiment to look at sentiment of bitcion related tweets\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "vs = analyzer.polarity_scores(\"this is a good example\")\n",
    "print(str(vs))\n",
    "print(str(vs[\"compound\"]))\n",
    "\n",
    "directory = r\"C:\\\\Users\\\\Dante\\Documents\\\\Stanford\\\\Coursework\\\\Winter 2018\\\\CS230\\\\Project\\\\Sentiment\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run vaderSentiment of selected tweets. For this example, we chose tweets in November 2017\n",
    "total = []\n",
    "j=1\n",
    "for t in range(1,29):\n",
    "    tweets = pd.read_csv(directory + \"2017-11-\" + str(t) + \".csv\", delimiter = \";\")\n",
    "    tweets['text'].astype('str')\n",
    "    scores = []\n",
    "    for tweet in tweets['text']:\n",
    "        vs = analyzer.polarity_scores(tweet)\n",
    "        scores.append(vs[\"compound\"])\n",
    "        df = pd.DataFrame(np.array(scores),columns = [\"compound\"])\n",
    "    total_score = df[\"compound\"].sum()/20\n",
    "    total.append(total_score)\n",
    "\n",
    "total_sentiment = pd.DataFrame(np.array(total), columns = [\"compound\"])    \n",
    "print(total_sentiment)\n",
    "\n",
    "total_sentiment.to_csv(\"C:\\\\Users\\\\Dante\\\\Documents\\\\Stanford\\\\Coursework\\\\Winter 2018\\\\CS230\\\\Project\\\\total_sentiment.csv\")\n",
    "\n",
    "scores = []\n",
    "for tweet in tweets['text']:\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    scores.append(vs[\"compound\"])\n",
    "df = pd.DataFrame(np.array(scores),columns = [\"compound\"])\n",
    "x = df[\"compound\"].sum()/20\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot distrubtuion of sentiment\n",
    "plt.bar(range(1,total_sentiment['compound'].size+1),total_sentiment['compound'], color = 'g', linewidth = 2.0)\n",
    "plt.ylabel('Sentiment',fontsize=12)\n",
    "plt.xlabel('Day in November',fontsize=12)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell runs the model with Twitter Seniment Analysis for 28 days of Tweets\n",
    "# Didn't improve large gap between train/dev set. Larger amount of tweets could improve this\n",
    "## Inputs ##\n",
    "timesteps = 2\n",
    "n_a = 64\n",
    "X_width = 6\n",
    "batch_size = 6\n",
    "binner_settings = [-0.025, -0.01, -0.003, 0.003, 0.01, 0.025]\n",
    "loss = \"categorical_crossentropy\"\n",
    "\n",
    "BTC_Price = import_data_file(directory, bitcoin_data_filename)\n",
    "Twitter_sentiment = pd.read_csv(directory + \"/total_sentiment.csv\")\n",
    "\n",
    "X_daily = generate_daily_dataset(BTC_Price, 1093)\n",
    "delta = X_daily[\"Close\"] - X_daily[\"Open\"]\n",
    "percent_delta = np.divide(delta, X_daily[\"Open\"])\n",
    "Y_daily = generate_binned_labels_custom(percent_delta, binner_settings)\n",
    "\n",
    "X = X_daily.as_matrix()\n",
    "Y = Y_daily.as_matrix()\n",
    "\n",
    "# Get one month to line up with Twitter Data\n",
    "X_one_month_dummy = np.zeros((28, X_width + 1))\n",
    "X_one_month = np.zeros((28, X_width + 2))\n",
    "j = 0\n",
    "for i in range (0,len(X)):\n",
    "    if X[i, 0] >= 1509504315 and X[i, 0] <= 1511923515:\n",
    "        X_one_month_dummy[j, :] = X[i, :]\n",
    "        j += 1\n",
    "        \n",
    "X_one_month[:, 0:7] = X_one_month_dummy    \n",
    "X_one_month[:, X_width+1] = Twitter_sentiment[\"compound\"]\n",
    "\n",
    "X = rearrange_training_data(X_one_month, timesteps, X_width)\n",
    "X_train = X[0:18,:,:]\n",
    "X_dev = X[19:25,:,:]\n",
    "\n",
    "Y_train = Y[1+timesteps:1+timesteps+18,:]\n",
    "Y_dev = Y[19+1+timesteps:25+1+timesteps,:]\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "plt.bar(linspace(0, len(binner_settings), len(binner_settings)+1), np.sum(Y, axis = 0)/np.sum(np.sum(Y, axis = 0)))\n",
    "plt.ylabel('Frequency (%)')\n",
    "plt.xlabel('Bin Number')\n",
    "plt.show()\n",
    "\n",
    "model = price_data_binned_model(timesteps, 6, n_a, len(binner_settings)+1, batch_size, loss)\n",
    "model.summary()\n",
    "training = model.fit(X_train, Y_train, epochs = 1000, batch_size = batch_size, validation_data=(X_dev, Y_dev))\n",
    "generate_plots(training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
